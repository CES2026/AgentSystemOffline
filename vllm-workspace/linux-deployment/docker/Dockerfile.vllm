# vLLM Server Dockerfile
# 支持 INT4 AWQ 量化的 Llama-3.1-70B 模型
# GPU 要求：1 x A100-80GB 或类似 GPU

FROM nvidia/cuda:12.1.0-devel-ubuntu22.04

# 设置环境变量
ENV DEBIAN_FRONTEND=noninteractive \
    PYTHONUNBUFFERED=1 \
    CUDA_HOME=/usr/local/cuda \
    PATH=/usr/local/cuda/bin:$PATH \
    LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH

# 安装系统依赖
RUN apt-get update && apt-get install -y \
    python3.10 \
    python3.10-dev \
    python3-pip \
    git \
    curl \
    wget \
    vim \
    build-essential \
    && rm -rf /var/lib/apt/lists/*

# 设置 Python 3.10 为默认 python
RUN update-alternatives --install /usr/bin/python python /usr/bin/python3.10 1 && \
    update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.10 1 && \
    update-alternatives --install /usr/bin/pip pip /usr/bin/pip3 1

# 升级 pip
RUN pip install --no-cache-dir --upgrade pip setuptools wheel

# 安装 PyTorch（CUDA 12.1 版本）
RUN pip install --no-cache-dir \
    torch==2.5.1 \
    torchvision \
    torchaudio \
    --index-url https://download.pytorch.org/whl/cu121

# 安装 vLLM 和相关依赖
RUN pip install --no-cache-dir \
    vllm==0.6.6.post1 \
    transformers==4.46.0 \
    accelerate \
    hf-transfer \
    sentencepiece \
    protobuf

# 安装 AWQ 量化支持
RUN pip install --no-cache-dir \
    autoawq \
    autoawq-kernels

# 创建工作目录
WORKDIR /app

# 创建模型缓存目录
RUN mkdir -p /models

# 设置 Hugging Face 缓存目录
ENV HF_HOME=/models
ENV TRANSFORMERS_CACHE=/models

# 复制配置文件（如果有）
COPY config/* /config/ 2>/dev/null || true

# 健康检查脚本
RUN echo '#!/bin/bash\ncurl -f http://localhost:8000/health || exit 1' > /healthcheck.sh && \
    chmod +x /healthcheck.sh

# 暴露端口
EXPOSE 8000

# 启动脚本
COPY docker/start-vllm.sh /start-vllm.sh
RUN chmod +x /start-vllm.sh

# 启动命令
CMD ["/start-vllm.sh"]
