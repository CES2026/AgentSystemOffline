#!/bin/bash
set -e

echo "============================================"
echo "ğŸš€ Starting vLLM Server (INT4 AWQ Quantized)"
echo "============================================"

# æ˜¾ç¤ºé…ç½®
echo "ğŸ“¦ Model: ${VLLM_MODEL:-casperhansen/llama-3.1-70b-instruct-awq}"
echo "ğŸ® GPU Memory Utilization: ${VLLM_GPU_MEMORY_UTILIZATION:-0.90}"
echo "ğŸ’¾ Max Model Length: ${VLLM_MAX_MODEL_LEN:-8192}"
echo "ğŸ”§ Tensor Parallel Size: ${VLLM_TENSOR_PARALLEL:-1}"
echo "ğŸ“Š Quantization: ${VLLM_QUANTIZATION:-awq}"
echo "============================================"

# æ£€æŸ¥ GPU å¯ç”¨æ€§
if ! nvidia-smi &> /dev/null; then
    echo "âŒ Error: No NVIDIA GPU detected!"
    echo "Please ensure:"
    echo "  1. NVIDIA drivers are installed"
    echo "  2. Docker is configured with GPU support (nvidia-docker2)"
    echo "  3. docker-compose.yml has GPU reservations configured"
    exit 1
fi

echo ""
echo "âœ… GPU Information:"
nvidia-smi --query-gpu=name,memory.total,driver_version --format=csv,noheader
echo ""

# æ£€æŸ¥ Hugging Face Tokenï¼ˆç”¨äºä¸‹è½½æ¨¡å‹ï¼‰
if [ -z "$HUGGING_FACE_HUB_TOKEN" ]; then
    echo "âš ï¸  Warning: HUGGING_FACE_HUB_TOKEN not set"
    echo "   Some models may not be accessible"
    echo ""
fi

# è®¾ç½®é»˜è®¤å€¼
VLLM_MODEL="${VLLM_MODEL:-casperhansen/llama-3.1-70b-instruct-awq}"
VLLM_PORT="${VLLM_PORT:-8000}"
VLLM_HOST="${VLLM_HOST:-0.0.0.0}"
VLLM_MAX_MODEL_LEN="${VLLM_MAX_MODEL_LEN:-8192}"
VLLM_GPU_MEMORY_UTILIZATION="${VLLM_GPU_MEMORY_UTILIZATION:-0.90}"
VLLM_TENSOR_PARALLEL="${VLLM_TENSOR_PARALLEL:-1}"
VLLM_QUANTIZATION="${VLLM_QUANTIZATION:-awq}"

# æ„å»ºå¯åŠ¨å‘½ä»¤
CMD="python3 -m vllm.entrypoints.openai.api_server"
CMD="$CMD --model $VLLM_MODEL"
CMD="$CMD --port $VLLM_PORT"
CMD="$CMD --host $VLLM_HOST"
CMD="$CMD --max-model-len $VLLM_MAX_MODEL_LEN"
CMD="$CMD --gpu-memory-utilization $VLLM_GPU_MEMORY_UTILIZATION"
CMD="$CMD --tensor-parallel-size $VLLM_TENSOR_PARALLEL"
CMD="$CMD --quantization $VLLM_QUANTIZATION"
CMD="$CMD --download-dir /models"

# æ·»åŠ å¯é€‰çš„ API Key
if [ -n "$VLLM_SERVER_API_KEY" ]; then
    CMD="$CMD --api-key $VLLM_SERVER_API_KEY"
    echo "ğŸ” API Key authentication enabled"
fi

# å¯ç”¨ HF Transfer åŠ é€Ÿä¸‹è½½
export HF_HUB_ENABLE_HF_TRANSFER=1

echo ""
echo "ğŸ”¨ Launch command:"
echo "$CMD"
echo ""
echo "â³ Starting vLLM server (this may take several minutes for first-time model download)..."
echo ""

# æ‰§è¡Œå¯åŠ¨å‘½ä»¤
exec $CMD
