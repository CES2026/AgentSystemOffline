# ==========================================
# vLLM Linux 部署配置文件
# ==========================================
# 使用方法：
#   1. 复制此文件为 .env: cp .env.example .env
#   2. 填写必需的配置项
#   3. 运行: docker-compose up -d

# ==========================================
# 必需配置
# ==========================================

# Hugging Face Token（必需 - 用于下载模型）
# 获取方式: https://huggingface.co/settings/tokens
HUGGING_FACE_HUB_TOKEN=your_huggingface_token_here

# ==========================================
# 模型配置
# ==========================================

# 模型名称（INT4 AWQ 量化版本）
# 推荐: casperhansen/llama-3.1-70b-instruct-awq (单 GPU，~55GB 显存)
# 备选: meta-llama/Llama-3.1-8B-Instruct (测试用，~8GB 显存)
VLLM_MODEL=casperhansen/llama-3.1-70b-instruct-awq

# 最大上下文长度（tokens）
# 8192: 推荐，平衡性能和内存
# 16384: 长上下文，需要更多显存
VLLM_MAX_MODEL_LEN=8192

# GPU 显存利用率（0.0 - 1.0）
# 0.90: 推荐，激进但稳定
# 0.85: 保守，留更多缓冲
VLLM_GPU_MEMORY_UTILIZATION=0.90

# Tensor Parallel 大小（GPU 数量）
# 1: 单 GPU（INT4 量化版本）
# 2: 双 GPU（FP16 完整版本）
VLLM_TENSOR_PARALLEL=1

# ==========================================
# 存储配置
# ==========================================

# 模型缓存路径（绝对路径）
# 确保有足够空间：70B INT4 需要 ~55-60GB
VLLM_MODEL_CACHE_PATH=/data/vllm-models

# ==========================================
# 安全配置（可选）
# ==========================================

# vLLM 服务器 API Key（可选 - 保护 vLLM 端口）
# 留空则不启用认证
VLLM_SERVER_API_KEY=

# FastAPI Wrapper API Key（可选 - 保护对外接口）
# 留空则不启用认证
VLLM_WRAPPER_API_KEY=

# ==========================================
# 高级配置（通常不需要修改）
# ==========================================

# 默认最大生成 tokens
VLLM_DEFAULT_MAX_TOKENS=2048

# 默认温度参数
VLLM_DEFAULT_TEMPERATURE=0.7

# ==========================================
# 网络端口（仅在端口冲突时修改）
# ==========================================

# vLLM 服务器端口（内网）
# VLLM_PORT=8000

# FastAPI Wrapper 端口（对外）
# WRAPPER_PORT=8001
