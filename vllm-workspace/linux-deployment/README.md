# vLLM Linux æœåŠ¡å™¨éƒ¨ç½²æŒ‡å—

æœ¬ç›®å½•åŒ…å«äº†å°† Modal éƒ¨ç½²çš„ vLLM è¿ç§»åˆ° Linux æœåŠ¡å™¨çš„å®Œæ•´éƒ¨ç½²æ–¹æ¡ˆã€‚

## ğŸ“‹ ç›®å½•

- [ç³»ç»Ÿè¦æ±‚](#ç³»ç»Ÿè¦æ±‚)
- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
- [è¯¦ç»†é…ç½®](#è¯¦ç»†é…ç½®)
- [ç®¡ç†å‘½ä»¤](#ç®¡ç†å‘½ä»¤)
- [æ•…éšœæ’æŸ¥](#æ•…éšœæ’æŸ¥)
- [æ€§èƒ½ä¼˜åŒ–](#æ€§èƒ½ä¼˜åŒ–)

---

## ğŸ–¥ï¸ ç³»ç»Ÿè¦æ±‚

### ç¡¬ä»¶è¦æ±‚

**GPU é…ç½®ï¼ˆæ ¹æ®æ¨¡å‹é€‰æ‹©ï¼‰ï¼š**

| æ¨¡å‹ | GPU éœ€æ±‚ | æ˜¾å­˜ | å­˜å‚¨ç©ºé—´ |
|------|---------|------|---------|
| **Llama-3.1-70B INT4 AWQï¼ˆæ¨èï¼‰** | 1x A100-80GB | ~55GB | ~60GB |
| Llama-3.1-70B FP16 | 2x A100-80GB | ~140GB | ~150GB |
| Llama-3.1-8B | 1x A100-40GB | ~8GB | ~20GB |

**å…¶ä»–ç¡¬ä»¶ï¼š**
- CPU: 8+ æ ¸å¿ƒ
- å†…å­˜: 32GB+ RAM
- å­˜å‚¨: 100GB+ SSDï¼ˆç”¨äºæ¨¡å‹ç¼“å­˜ï¼‰
- ç½‘ç»œ: 100+ Mbpsï¼ˆé¦–æ¬¡ä¸‹è½½æ¨¡å‹ï¼‰

### è½¯ä»¶è¦æ±‚

- **æ“ä½œç³»ç»Ÿ**: Ubuntu 20.04+ / Debian 11+ / CentOS 8+
- **NVIDIA é©±åŠ¨**: 525.60.13+ (æ”¯æŒ CUDA 12.1)
- **Docker**: 20.10+
- **Docker Compose**: 2.0+
- **nvidia-docker2**: æœ€æ–°ç‰ˆæœ¬

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–

#### Ubuntu/Debian

```bash
# å®‰è£… Docker
curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh
sudo usermod -aG docker $USER

# å®‰è£… NVIDIA é©±åŠ¨ï¼ˆå¦‚æœæœªå®‰è£…ï¼‰
sudo ubuntu-drivers autoinstall

# å®‰è£… nvidia-docker2
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg
curl -s -L https://nvidia.github.io/libnvidia-container/$distribution/libnvidia-container.list | \
    sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \
    sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list

sudo apt-get update
sudo apt-get install -y nvidia-docker2
sudo systemctl restart docker

# éªŒè¯ GPU æ”¯æŒ
docker run --rm --gpus all nvidia/cuda:12.1.0-base-ubuntu22.04 nvidia-smi
```

### 2. å…‹éš†æˆ–å¤åˆ¶éƒ¨ç½²æ–‡ä»¶

```bash
cd /path/to/deployment
# ç¡®ä¿åœ¨ linux-deployment ç›®å½•ä¸‹
ls  # åº”è¯¥çœ‹åˆ°: docker-compose.yml, .env.example, docker/, scripts/, config/
```

### 3. é…ç½®ç¯å¢ƒå˜é‡

```bash
# å¤åˆ¶é…ç½®æ–‡ä»¶
cp .env.example .env

# ç¼–è¾‘é…ç½®æ–‡ä»¶
nano .env  # æˆ–ä½¿ç”¨ vim

# å¿…é¡»è®¾ç½®ï¼š
# - HUGGING_FACE_HUB_TOKEN: ä» https://huggingface.co/settings/tokens è·å–
# - VLLM_MODEL_CACHE_PATH: æ¨¡å‹ç¼“å­˜è·¯å¾„ï¼ˆç¡®ä¿æœ‰è¶³å¤Ÿç©ºé—´ï¼‰
```

**æœ€å°é…ç½®ç¤ºä¾‹ï¼š**

```bash
# .env
HUGGING_FACE_HUB_TOKEN=hf_xxxxxxxxxxxxxxxxxxxx
VLLM_MODEL=casperhansen/llama-3.1-70b-instruct-awq
VLLM_MODEL_CACHE_PATH=/data/vllm-models
```

### 4. å¯åŠ¨æœåŠ¡

```bash
# è¿è¡Œå¯åŠ¨è„šæœ¬ï¼ˆä¼šè‡ªåŠ¨æ£€æŸ¥ç¯å¢ƒï¼‰
./scripts/start.sh

# æˆ–ç›´æ¥ä½¿ç”¨ docker-compose
docker-compose up -d
```

### 5. éªŒè¯éƒ¨ç½²

```bash
# å¥åº·æ£€æŸ¥
./scripts/health-check.sh

# æŸ¥çœ‹æ—¥å¿—
./scripts/logs.sh

# æµ‹è¯• API
./scripts/test-api.sh
```

**é¢„æœŸè¾“å‡ºï¼š**

```
ğŸ¥ vLLM æœåŠ¡å¥åº·æ£€æŸ¥
============================================

ğŸ“¦ å®¹å™¨çŠ¶æ€:
NAME            STATE     PORTS
vllm-server     running   0.0.0.0:8000->8000/tcp
vllm-wrapper    running   0.0.0.0:8001->8001/tcp

ğŸ” æ£€æŸ¥ vLLM æœåŠ¡å™¨ (http://localhost:8000)...
âœ… vLLM æœåŠ¡å™¨è¿è¡Œæ­£å¸¸

ğŸ” æ£€æŸ¥ FastAPI Wrapper (http://localhost:8001)...
âœ… FastAPI Wrapper è¿è¡Œæ­£å¸¸
```

---

## âš™ï¸ è¯¦ç»†é…ç½®

### ç¯å¢ƒå˜é‡è¯´æ˜

å®Œæ•´çš„ç¯å¢ƒå˜é‡é…ç½®è¯·å‚è€ƒ `.env.example`ã€‚ä»¥ä¸‹æ˜¯å…³é”®é…ç½®ï¼š

#### æ¨¡å‹é…ç½®

```bash
# æ¨¡å‹é€‰æ‹©
VLLM_MODEL=casperhansen/llama-3.1-70b-instruct-awq

# ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆtokensï¼‰
VLLM_MAX_MODEL_LEN=8192  # 8K ä¸Šä¸‹æ–‡
# VLLM_MAX_MODEL_LEN=16384  # 16K ä¸Šä¸‹æ–‡ï¼ˆéœ€è¦æ›´å¤šæ˜¾å­˜ï¼‰

# GPU æ˜¾å­˜åˆ©ç”¨ç‡ï¼ˆ0.0-1.0ï¼‰
VLLM_GPU_MEMORY_UTILIZATION=0.90  # æ¿€è¿›ä½†ç¨³å®š
# VLLM_GPU_MEMORY_UTILIZATION=0.85  # ä¿å®ˆï¼Œæ›´å¤šç¼“å†²
```

#### å®‰å…¨é…ç½®ï¼ˆå¯é€‰ï¼‰

```bash
# å¯ç”¨ API è®¤è¯
VLLM_WRAPPER_API_KEY=your-secret-api-key

# ä½¿ç”¨æ—¶éœ€è¦åœ¨è¯·æ±‚å¤´ä¸­æ·»åŠ ï¼š
# Authorization: Bearer your-secret-api-key
```

#### æ€§èƒ½é…ç½®

```bash
# Tensor Parallelï¼ˆå¤š GPU å¹¶è¡Œï¼‰
VLLM_TENSOR_PARALLEL=1  # å• GPU
# VLLM_TENSOR_PARALLEL=2  # åŒ GPUï¼ˆFP16 å®Œæ•´æ¨¡å‹ï¼‰

# é»˜è®¤ç”Ÿæˆå‚æ•°
VLLM_DEFAULT_MAX_TOKENS=2048
VLLM_DEFAULT_TEMPERATURE=0.7
```

### Docker Compose é…ç½®

å¦‚éœ€ä¿®æ”¹ç«¯å£æˆ–èµ„æºé™åˆ¶ï¼Œç¼–è¾‘ `docker-compose.yml`ï¼š

```yaml
# ä¿®æ”¹ç«¯å£æ˜ å°„
services:
  vllm-server:
    ports:
      - "9000:8000"  # æ”¹ä¸º 9000 ç«¯å£

  fastapi-wrapper:
    ports:
      - "9001:8001"  # æ”¹ä¸º 9001 ç«¯å£
```

---

## ğŸ”§ ç®¡ç†å‘½ä»¤

æ‰€æœ‰ç®¡ç†è„šæœ¬ä½äº `scripts/` ç›®å½•ï¼š

```bash
# å¯åŠ¨æœåŠ¡
./scripts/start.sh

# åœæ­¢æœåŠ¡
./scripts/stop.sh

# é‡å¯æœåŠ¡
./scripts/restart.sh

# æŸ¥çœ‹æ—¥å¿—ï¼ˆæ‰€æœ‰æœåŠ¡ï¼‰
./scripts/logs.sh

# æŸ¥çœ‹ç‰¹å®šæœåŠ¡æ—¥å¿—
./scripts/logs.sh vllm-server   # vLLM æœåŠ¡å™¨æ—¥å¿—
./scripts/logs.sh vllm-wrapper  # FastAPI Wrapper æ—¥å¿—

# å¥åº·æ£€æŸ¥
./scripts/health-check.sh

# API æµ‹è¯•
./scripts/test-api.sh
```

### æ‰‹åŠ¨å‘½ä»¤

```bash
# æŸ¥çœ‹å®¹å™¨çŠ¶æ€
docker-compose ps

# æŸ¥çœ‹èµ„æºä½¿ç”¨
docker stats

# è¿›å…¥å®¹å™¨
docker exec -it vllm-server bash
docker exec -it vllm-wrapper bash

# æ¸…ç†å¹¶é‡å»º
docker-compose down
docker-compose build --no-cache
docker-compose up -d

# å®Œå…¨æ¸…ç†ï¼ˆåŒ…æ‹¬æ¨¡å‹ç¼“å­˜å·ï¼‰
docker-compose down -v
```

---

## ğŸ“¡ API ä½¿ç”¨

éƒ¨ç½²æˆåŠŸåï¼Œå¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼è®¿é—® APIï¼š

### 1. FastAPI æ–‡æ¡£ï¼ˆæ¨èï¼‰

è®¿é—®ï¼š`http://localhost:8001/docs`

### 2. curl ç¤ºä¾‹

```bash
# å¥åº·æ£€æŸ¥
curl http://localhost:8001/health

# ç®€å•å¯¹è¯
curl -X POST http://localhost:8001/chat \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "user", "content": "ä½ å¥½"}
    ],
    "max_tokens": 100
  }'

# OpenAI å…¼å®¹æ¥å£
curl -X POST http://localhost:8001/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "casperhansen/llama-3.1-70b-instruct-awq",
    "messages": [
      {"role": "user", "content": "Hello"}
    ],
    "max_tokens": 100
  }'

# æµå¼å“åº”
curl -X POST http://localhost:8001/chat \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "user", "content": "æ•°åˆ°10"}
    ],
    "stream": true
  }'
```

### 3. Python ç¤ºä¾‹

```python
from openai import OpenAI

# ä½¿ç”¨ OpenAI SDKï¼ˆå…¼å®¹ï¼‰
client = OpenAI(
    base_url="http://localhost:8001/v1",
    api_key="EMPTY"  # å¦‚æœè®¾ç½®äº† VLLM_WRAPPER_API_KEYï¼Œå¡«å†™å®é™…å€¼
)

response = client.chat.completions.create(
    model="casperhansen/llama-3.1-70b-instruct-awq",
    messages=[
        {"role": "user", "content": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±"}
    ],
    max_tokens=200,
    temperature=0.7
)

print(response.choices[0].message.content)
```

---

## ğŸ› æ•…éšœæ’æŸ¥

### é—®é¢˜ 1: GPU ä¸å¯ç”¨

**ç—‡çŠ¶ï¼š**
```
Error: No NVIDIA GPU detected!
```

**è§£å†³æ–¹æ¡ˆï¼š**

```bash
# 1. æ£€æŸ¥ NVIDIA é©±åŠ¨
nvidia-smi

# 2. æ£€æŸ¥ Docker GPU æ”¯æŒ
docker run --rm --gpus all nvidia/cuda:12.1.0-base-ubuntu22.04 nvidia-smi

# 3. é‡å¯ Docker
sudo systemctl restart docker

# 4. é‡æ–°å®‰è£… nvidia-docker2
sudo apt-get install --reinstall nvidia-docker2
sudo systemctl restart docker
```

### é—®é¢˜ 2: æ¨¡å‹ä¸‹è½½å¤±è´¥

**ç—‡çŠ¶ï¼š**
```
HTTPError: 401 Unauthorized
```

**è§£å†³æ–¹æ¡ˆï¼š**

1. æ£€æŸ¥ Hugging Face Token æ˜¯å¦æ­£ç¡®
2. ç¡®è®¤ Token æœ‰è¶³å¤Ÿæƒé™ï¼ˆéœ€è¦ read æƒé™ï¼‰
3. å¯¹äº Llama æ¨¡å‹ï¼Œéœ€è¦å…ˆç”³è¯·è®¿é—®æƒé™ï¼šhttps://huggingface.co/meta-llama

```bash
# æµ‹è¯• Token æ˜¯å¦æœ‰æ•ˆ
curl -H "Authorization: Bearer YOUR_TOKEN" \
  https://huggingface.co/api/whoami
```

### é—®é¢˜ 3: æ˜¾å­˜ä¸è¶³

**ç—‡çŠ¶ï¼š**
```
OutOfMemoryError: CUDA out of memory
```

**è§£å†³æ–¹æ¡ˆï¼š**

```bash
# æ–¹æ¡ˆ 1: é™ä½æ˜¾å­˜åˆ©ç”¨ç‡
# åœ¨ .env ä¸­è®¾ç½®
VLLM_GPU_MEMORY_UTILIZATION=0.80  # ä» 0.90 é™åˆ° 0.80

# æ–¹æ¡ˆ 2: å‡å°ä¸Šä¸‹æ–‡é•¿åº¦
VLLM_MAX_MODEL_LEN=4096  # ä» 8192 é™åˆ° 4096

# æ–¹æ¡ˆ 3: ä½¿ç”¨æ›´å°çš„æ¨¡å‹
VLLM_MODEL=meta-llama/Llama-3.1-8B-Instruct

# é‡å¯æœåŠ¡
./scripts/restart.sh
```

### é—®é¢˜ 4: å®¹å™¨å¯åŠ¨æ…¢

**ç—‡çŠ¶ï¼š**
é¦–æ¬¡å¯åŠ¨ç­‰å¾…å¾ˆä¹…ï¼ŒvLLM æœåŠ¡å™¨ä¸€ç›´æ˜¾ç¤º "Starting..."

**åŸå› ï¼š**
æ­£åœ¨ä¸‹è½½æ¨¡å‹ï¼ˆ70B INT4 çº¦ 55GBï¼‰

**è§£å†³æ–¹æ¡ˆï¼š**

```bash
# æŸ¥çœ‹ä¸‹è½½è¿›åº¦
./scripts/logs.sh vllm-server

# é¢„æœŸçœ‹åˆ°ï¼š
# Downloading model... 5.2GB/55GB
# ä¸‹è½½é€Ÿåº¦å–å†³äºç½‘ç»œå’Œ Hugging Face æœåŠ¡å™¨

# è€å¿ƒç­‰å¾…ï¼Œé¦–æ¬¡ä¸‹è½½éœ€è¦ 10-30 åˆ†é’Ÿ
```

### é—®é¢˜ 5: API å“åº”æ…¢

**å¯èƒ½åŸå› ï¼š**

1. **é¦–æ¬¡æ¨ç†**: vLLM éœ€è¦é¢„çƒ­ï¼ˆæ­£å¸¸ï¼‰
2. **ä¸Šä¸‹æ–‡è¿‡é•¿**: å‡å° max_tokens æˆ–ä¸Šä¸‹æ–‡é•¿åº¦
3. **èµ„æºä¸è¶³**: æ£€æŸ¥ GPU/CPU/å†…å­˜ä½¿ç”¨ç‡

```bash
# æŸ¥çœ‹èµ„æºä½¿ç”¨
docker stats

# æŸ¥çœ‹ GPU ä½¿ç”¨
nvidia-smi

# ä¼˜åŒ–å‚æ•°
# åœ¨è¯·æ±‚ä¸­è®¾ç½®è¾ƒå°çš„ max_tokens
curl -X POST http://localhost:8001/chat \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [...],
    "max_tokens": 512  # å‡å°ç”Ÿæˆé•¿åº¦
  }'
```

---

## âš¡ æ€§èƒ½ä¼˜åŒ–

### 1. æ¨¡å‹ç¼“å­˜ä¼˜åŒ–

å°†æ¨¡å‹ç¼“å­˜ç›®å½•æ”¾åœ¨é«˜é€Ÿ SSD ä¸Šï¼š

```bash
# .env
VLLM_MODEL_CACHE_PATH=/nvme/vllm-models  # ä½¿ç”¨ NVMe SSD
```

### 2. ç½‘ç»œä¼˜åŒ–

å¯¹äºç”Ÿäº§ç¯å¢ƒï¼Œå»ºè®®ä½¿ç”¨ Nginx åå‘ä»£ç†ï¼š

```nginx
# /etc/nginx/sites-available/vllm
server {
    listen 80;
    server_name your-domain.com;

    location / {
        proxy_pass http://localhost:8001;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_buffering off;  # é‡è¦ï¼šæ”¯æŒæµå¼å“åº”
    }
}
```

### 3. å¹¶å‘ä¼˜åŒ–

FastAPI Wrapper æ”¯æŒå¤š workerï¼š

```bash
# ä¿®æ”¹ docker/Dockerfile.wrapper çš„å¯åŠ¨å‘½ä»¤
CMD ["python", "-m", "uvicorn", "vllm_wrapper:app", \
     "--host", "0.0.0.0", "--port", "8001", \
     "--workers", "4"]  # å¢åŠ  worker æ•°é‡
```

### 4. ç›‘æ§å’Œæ—¥å¿—

ä½¿ç”¨ Prometheus + Grafana ç›‘æ§ï¼š

```bash
# æ·»åŠ åˆ° docker-compose.yml
services:
  prometheus:
    image: prom/prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./config/prometheus.yml:/etc/prometheus/prometheus.yml

  grafana:
    image: grafana/grafana
    ports:
      - "3000:3000"
```

---

## ğŸ“Š èµ„æºç›‘æ§

### GPU ä½¿ç”¨

```bash
# å®æ—¶ç›‘æ§
watch -n 1 nvidia-smi

# æŸ¥çœ‹ GPU åˆ©ç”¨ç‡å†å²
nvidia-smi dmon -s u
```

### å®¹å™¨èµ„æº

```bash
# æŸ¥çœ‹èµ„æºä½¿ç”¨
docker stats vllm-server vllm-wrapper

# æŸ¥çœ‹è¯¦ç»†ä¿¡æ¯
docker inspect vllm-server | jq '.[0].State'
```

---

## ğŸ” ç”Ÿäº§ç¯å¢ƒå»ºè®®

### 1. å¯ç”¨ API è®¤è¯

```bash
# .env
VLLM_WRAPPER_API_KEY=your-strong-api-key-here
```

### 2. ä½¿ç”¨ HTTPS

é€šè¿‡ Nginx + Let's Encrypt æ·»åŠ  SSLï¼š

```bash
sudo apt-get install certbot python3-certbot-nginx
sudo certbot --nginx -d your-domain.com
```

### 3. é™æµå’Œè´Ÿè½½å‡è¡¡

ä½¿ç”¨ Nginx é™æµï¼š

```nginx
# é™åˆ¶è¯·æ±‚é€Ÿç‡
limit_req_zone $binary_remote_addr zone=api_limit:10m rate=10r/s;

location /chat {
    limit_req zone=api_limit burst=20 nodelay;
    proxy_pass http://localhost:8001;
}
```

### 4. æ—¥å¿—ç®¡ç†

é…ç½®æ—¥å¿—è½®è½¬ï¼š

```bash
# /etc/logrotate.d/docker-vllm
/var/lib/docker/containers/*/*.log {
    daily
    rotate 7
    compress
    missingok
    notifempty
}
```

---

## ğŸ“š æ¶æ„å¯¹æ¯”

### Modal vs Linux éƒ¨ç½²

| ç‰¹æ€§ | Modal éƒ¨ç½² | Linux éƒ¨ç½² |
|------|-----------|-----------|
| **éƒ¨ç½²éš¾åº¦** | ç®€å•ï¼ˆä¸€é”®éƒ¨ç½²ï¼‰ | ä¸­ç­‰ï¼ˆéœ€è¦é…ç½®ï¼‰ |
| **æˆæœ¬** | æŒ‰ä½¿ç”¨ä»˜è´¹ | å›ºå®šæˆæœ¬ï¼ˆæœåŠ¡å™¨ï¼‰ |
| **è‡ªåŠ¨æ‰©å±•** | æ”¯æŒ | éœ€æ‰‹åŠ¨é…ç½® |
| **ç»´æŠ¤** | æ‰˜ç®¡ï¼ˆæ— éœ€ç»´æŠ¤ï¼‰ | è‡ªè¡Œç»´æŠ¤ |
| **æ§åˆ¶æƒ** | æœ‰é™ | å®Œå…¨æ§åˆ¶ |
| **ç½‘ç»œå»¶è¿Ÿ** | å–å†³äº Modal åŒºåŸŸ | æœ¬åœ°ç½‘ç»œï¼ˆæœ€ä½ï¼‰ |

---

## ğŸ†˜ è·å–å¸®åŠ©

å¦‚é‡åˆ°é—®é¢˜ï¼Œè¯·æ£€æŸ¥ï¼š

1. **æ—¥å¿—**: `./scripts/logs.sh`
2. **å¥åº·æ£€æŸ¥**: `./scripts/health-check.sh`
3. **GPU çŠ¶æ€**: `nvidia-smi`
4. **å®¹å™¨çŠ¶æ€**: `docker-compose ps`

å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆè¯·å‚è€ƒ [æ•…éšœæ’æŸ¥](#æ•…éšœæ’æŸ¥) ç« èŠ‚ã€‚

---

## ğŸ“„ è®¸å¯è¯

æœ¬éƒ¨ç½²æ–¹æ¡ˆåŸºäºåŸ Modal éƒ¨ç½²ä»£ç æ”¹ç¼–ï¼Œä¿ç•™åŸæœ‰è®¸å¯è¯ã€‚

---

**ç¥éƒ¨ç½²é¡ºåˆ©ï¼** ğŸ‰
